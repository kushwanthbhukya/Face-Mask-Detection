{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACEMASK DETECTION USING CONVOLUTION NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1: Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset--> Cnn Classifier(the classifier tha we are using today is VGG16) --> training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. data perpration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "# what is os? - operating system\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "print(tf.__version__)\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2 #cv2 is a computer vision library.\n",
    "from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['with_mask','without_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/without_mask/.DS_Store\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(len(categories)):\n",
    "    path = os.path.join('train',categories[i])\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path,img)\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "        except Exception:\n",
    "            print(img_path)\n",
    "        try:\n",
    "            img = cv2.resize(img,(224,224)) #resize the image to 224*224\n",
    "            data.append([img,i])  #append the image and the label to the data\n",
    "        except Exception:\n",
    "            print(img_path) #print the image path if the image is not loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/_gjv6z015jb2q776rtfhlns00000gn/T/ipykernel_1313/3146378798.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.array(data)#convert the data to numpy array\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.array(data)#convert the data to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1508, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to shuffle the dataset because we have continous data of with mask and without mask so we need to shufle\n",
    "# the dataset inorder to avoid bias.\n",
    "import random\n",
    "random.shuffle(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset into X,Y and converting them into numpy array.\n",
    "x=[]\n",
    "y=[]\n",
    "for features,label in data:\n",
    "    x.append(features) #appending the features to the x for training the model \n",
    "    y.append(label) #appending the label to the y for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1508"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1508"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=np.array(x) \n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1508, 224, 224, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape # we have in x 1508 images of size(224,224,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1508- number of images in x\\n224- height of image\\n224- width of image\\n3- number of channels(RGB)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1508- number of images in x\n",
    "224- height of image\n",
    "224- width of image\n",
    "3- number of channels(RGB)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1508,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape # we have in y 1508 images of size(1) why we have y is only 1508 is y is a lable data set and x os feature dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling of x\n",
    "x=x/255 #scaling the x data set to 0-1 in the range of 0-255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.54901961, 0.75686275, 0.84313725],\n",
       "        [0.61960784, 0.82745098, 0.89019608],\n",
       "        [0.69803922, 0.89019608, 0.9372549 ],\n",
       "        ...,\n",
       "        [0.73333333, 0.8       , 0.83529412],\n",
       "        [0.7254902 , 0.77647059, 0.80392157],\n",
       "        [0.12156863, 0.18039216, 0.18823529]],\n",
       "\n",
       "       [[0.60392157, 0.77647059, 0.85490196],\n",
       "        [0.65098039, 0.82352941, 0.89411765],\n",
       "        [0.67843137, 0.83921569, 0.90980392],\n",
       "        ...,\n",
       "        [0.6627451 , 0.77254902, 0.8       ],\n",
       "        [0.70196078, 0.79215686, 0.80392157],\n",
       "        [0.16470588, 0.23137255, 0.22352941]],\n",
       "\n",
       "       [[0.61568627, 0.78039216, 0.85098039],\n",
       "        [0.61960784, 0.77647059, 0.85882353],\n",
       "        [0.69411765, 0.83921569, 0.94117647],\n",
       "        ...,\n",
       "        [0.71764706, 0.87843137, 0.91372549],\n",
       "        [0.54509804, 0.68627451, 0.66666667],\n",
       "        [0.09411765, 0.2       , 0.1372549 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.02352941, 0.1254902 , 0.08627451],\n",
       "        [0.        , 0.05098039, 0.01960784],\n",
       "        [0.16470588, 0.21568627, 0.20784314],\n",
       "        ...,\n",
       "        [0.80392157, 0.80784314, 0.94117647],\n",
       "        [0.63921569, 0.70980392, 0.78039216],\n",
       "        [0.18039216, 0.27058824, 0.27058824]],\n",
       "\n",
       "       [[0.08235294, 0.21960784, 0.16078431],\n",
       "        [0.02745098, 0.16862745, 0.12156863],\n",
       "        [0.15294118, 0.23137255, 0.21176471],\n",
       "        ...,\n",
       "        [0.87058824, 0.83921569, 0.96470588],\n",
       "        [0.59215686, 0.63921569, 0.70980392],\n",
       "        [0.11372549, 0.17254902, 0.20392157]],\n",
       "\n",
       "       [[0.11372549, 0.27058824, 0.19607843],\n",
       "        [0.10196078, 0.30196078, 0.22745098],\n",
       "        [0.10980392, 0.22352941, 0.17647059],\n",
       "        ...,\n",
       "        [0.86666667, 0.78431373, 0.90196078],\n",
       "        [0.67843137, 0.67058824, 0.7372549 ],\n",
       "        [0.16470588, 0.15686275, 0.19607843]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1206, 224, 224, 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302, 224, 224, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 is object detection and classification algorithm which is able to classify 1000 images of 1000 different categories with 92.7% accuracy. It is one of the popular algorithms for image classification and is easy to use with transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an cnn  model ()\n",
    "model=Sequential()\n",
    "model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters=128,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters=128,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten()) #flatten is used to flat the complete array.\n",
    "model.add(Dense(units=256,activation='relu'))\n",
    "model.add(Dense(units=256,activation='relu'))\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "#what is the use of adam optimizer? - Adam is a stochastic gradient descent algorithm that is used to train the model.\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 11:34:50.673877: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - ETA: 0s - loss: 0.4171 - accuracy: 0.8433"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 11:35:28.061026: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 41s 913ms/step - loss: 0.4171 - accuracy: 0.8433 - val_loss: 0.2217 - val_accuracy: 0.9238\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 30s 785ms/step - loss: 0.1450 - accuracy: 0.9502 - val_loss: 0.1139 - val_accuracy: 0.9702\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 30s 790ms/step - loss: 0.2249 - accuracy: 0.9370 - val_loss: 0.1696 - val_accuracy: 0.9272\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 31s 812ms/step - loss: 0.1054 - accuracy: 0.9602 - val_loss: 0.1006 - val_accuracy: 0.9603\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 32s 847ms/step - loss: 0.0597 - accuracy: 0.9809 - val_loss: 0.0255 - val_accuracy: 0.9934\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,epochs=5,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 6,748,993\n",
      "Trainable params: 6,748,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created a model is done \n",
    "# now we need to predict  the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    img = cv2.imread(frame)\n",
    "    img = cv2.resize(img,(224,224))\n",
    "    cv2.imshow('window',frame)\n",
    "    y_pred = model.predict(img.reshape(-1,224,224,3))\n",
    "    print(y_pred)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face_mask(img):\n",
    "    y_pred=model.predict(img.reshape(1,224,224,3))\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81fae22719afd8004ae8f6d98c89b7951f8afce1af04c0e2d780aff91d37d1e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
